{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "a00e032c"
   },
   "source": [
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac36d3a",
   "metadata": {
    "id": "5ac36d3a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-4e69  GCE       4                                       RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "51cf86c5"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf199e6a",
   "metadata": {
    "id": "bf199e6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (1.22.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pointpats 2.2.0 requires opencv-contrib-python>=4.2.0, which is not installed.\n",
      "scipy 1.6.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
      "pysal 2.4.0 requires urllib3>=1.26, but you have urllib3 1.25.11 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed numpy-1.24.1\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: gensim==3.8.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim==3.8.3) (1.6.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim==3.8.3) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from gensim==3.8.3) (1.24.1)\n",
      "Collecting numpy>=1.11.3\n",
      "  Using cached numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pointpats 2.2.0 requires opencv-contrib-python>=4.2.0, which is not installed.\n",
      "pysal 2.4.0 requires urllib3>=1.26, but you have urllib3 1.25.11 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed numpy-1.22.4\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes\n",
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f56ecd",
   "metadata": {
    "id": "d8f56ecd",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors  \n",
    "from collections import defaultdict\n",
    "import math\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a897f2",
   "metadata": {
    "id": "38a897f2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan 12 12:23 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47900073",
   "metadata": {
    "id": "47900073",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5ad484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /opt/conda/miniconda3/lib/python3.8/site-packages (2.0.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bed56b",
   "metadata": {
    "id": "72bed56b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-spark-version",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-4e69-m.c.shirs-project-370310.internal:36479\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5704390c70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980e62a5",
   "metadata": {
    "id": "980e62a5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = '207437625' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "dir_n_title = \"postings_gcp_title\"\n",
    "dir_n_anchor = \"postings_gcp_anchor\"\n",
    "dir_n_text = \"postings_gcp_body\"\n",
    "dir_n_pr ='PageRank'\n",
    "dir_n_pv = 'Page_View'\n",
    "dir_n_w2v = 'Word2Vec'\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac891c2",
   "metadata": {
    "id": "cac891c2"
   },
   "source": [
    "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "582c3f5e"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c523e7",
   "metadata": {
    "id": "e4c523e7",
    "outputId": "eb0fe211-7396-42ce-d76a-da3567f16b4d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "titles_list = {x[1]: x[0] for x in doc_title_pairs.collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121fe102",
   "metadata": {
    "id": "121fe102",
    "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c101a8",
   "metadata": {
    "id": "57c101a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c259c402",
   "metadata": {
    "id": "c259c402"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "jAuL-XGm5dV3",
   "metadata": {
    "id": "jAuL-XGm5dV3"
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7h-0BVBi6EkQ",
   "metadata": {
    "id": "7h-0BVBi6EkQ"
   },
   "source": [
    "**word count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "O-cLHFRl5-8Q",
   "metadata": {
    "id": "O-cLHFRl5-8Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_count(text, id):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = [word for word in tokens if word not in all_stopwords]\n",
    "    counts = Counter(tokens)\n",
    "    tf_list = list()\n",
    "    for word in counts:\n",
    "        tf_list.append((word,(id,counts[word])))\n",
    "    return tf_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dqadqSFQ6Hkd",
   "metadata": {
    "id": "dqadqSFQ6Hkd"
   },
   "outputs": [],
   "source": [
    "def reduce_word_counts(unsorted_pl):\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8MZpOmmPnMJr",
   "metadata": {
    "id": "8MZpOmmPnMJr"
   },
   "outputs": [],
   "source": [
    "def fill_DL(text, id):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = [x for x in tokens if x not in all_stopwords]\n",
    "    return (id,len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hKyA7jxln04u",
   "metadata": {
    "id": "hKyA7jxln04u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DL= doc_text_pairs.map(lambda x:fill_DL(x[0], x[1]))\n",
    "DL=dict(DL.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac6217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "word_counts_text = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "word_counts_anchor=doc_anchor_pairs.map(lambda x:x[0])\n",
    "word_counts_anchor=word_counts_anchor.flatMap(lambda x:x)\n",
    "word_counts_anchor= word_counts_anchor.flatMap(lambda x:word_count(x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "EmPVJLMX6pS4",
   "metadata": {
    "id": "EmPVJLMX6pS4"
   },
   "outputs": [],
   "source": [
    "postings_text = word_counts_text.groupByKey().mapValues(reduce_word_counts)\n",
    "postings_anchor = word_counts_anchor.groupByKey().mapValues(list)\n",
    "postings_anchor =postings_anchor.map(lambda x : (x[0],sorted(list(set(x[1])))))\n",
    "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kp4jsm9z6ual",
   "metadata": {
    "id": "kp4jsm9z6ual"
   },
   "source": [
    "### Document frequency and filteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Zq3Gma46yZH",
   "metadata": {
    "id": "3Zq3Gma46yZH"
   },
   "source": [
    "Next, we will filter out rare words, words that appear in 10 or fewer documents\n",
    " **(when working on the entire corpus, we will increase this threshold to a minimum of 50 documents)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7HKz-pFK6se-",
   "metadata": {
    "id": "7HKz-pFK6se-"
   },
   "outputs": [],
   "source": [
    "postings_text = postings_text.filter(lambda x: len(x[1])>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Cu7RgrsM67Im",
   "metadata": {
    "id": "Cu7RgrsM67Im"
   },
   "outputs": [],
   "source": [
    "def calculate_df(postings):\n",
    "    return postings.map(lambda x:(x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "uNtuy0EG68Df",
   "metadata": {
    "id": "uNtuy0EG68Df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2df_text_dict = calculate_df(postings_text)\n",
    "w2df_text_dict = w2df_text_dict.collectAsMap()\n",
    "\n",
    "w2df_anchor_dict = calculate_df(postings_anchor)\n",
    "w2df_anchor_dict = w2df_anchor_dict.collectAsMap()\n",
    "\n",
    "w2df_title_dict = calculate_df(postings_title)\n",
    "w2df_title_dict = w2df_title_dict.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7812fb4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_doc_norm(text,id,N):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    tokens = [word for word in tokens if word not in all_stopwords]\n",
    "    count = Counter(tokens)\n",
    "    tfidf = defaultdict()\n",
    "    doc_len = len(tokens)\n",
    "    for word,tf in count.items():\n",
    "         if word in w2df_text_dict:##have more than 50 apperance\n",
    "            tfidf[word] = (tf/doc_len)*(math.log(N/w2df_text_dict[word],10))\n",
    "    norm =0\n",
    "    for word,tfidf in tfidf.items():\n",
    "        norm += tfidf**2\n",
    "    return (id,math.sqrt(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43574d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "N = len(DL)\n",
    "norm_text = doc_text_pairs.map(lambda x: calc_doc_norm(x[0], x[1],N))\n",
    "norm_dict =dict(norm_text.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4wn-mx-47IRl",
   "metadata": {
    "id": "4wn-mx-47IRl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Partitioning and writing the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "yaBxtaEL7KV5",
   "metadata": {
    "id": "yaBxtaEL7KV5"
   },
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token,dir_n):\n",
    "    if dir_n == \"postings_gcp_body\":\n",
    "        return int(_hash(token),16) % NUM_BUCKETS\n",
    "    if dir_n == \"postings_gcp_anchor\":\n",
    "        return int(_hash(token),16) % NUM_BUCKETS +124\n",
    "    if dir_n == \"postings_gcp_title\":\n",
    "        return int(_hash(token),16) % NUM_BUCKETS +248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdzLkwkD7NS8",
   "metadata": {
    "id": "cdzLkwkD7NS8"
   },
   "outputs": [],
   "source": [
    "def partition_postings_and_write(postings,dir_n):\n",
    "  # YOUR CODE HERE\n",
    "    postings = postings.map(lambda x: (token2bucket_id(x[0],dir_n),x)).groupByKey().mapValues(list)\n",
    "    return postings.map(lambda x: InvertedIndex(dir_n=dir_n).write_a_posting_list(x,bucket_name,dir_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#`collect` operator to aggregate the posting location information from the multiple buckets/indices into a single dictionary\n",
    "posting_locs_list_text = partition_postings_and_write(postings_text,dir_n_text).collect()\n",
    "# merge the posting locations into a single dict \n",
    "super_posting_locs_text = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=dir_n_text):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_text[k].extend(v)\n",
    "\n",
    "#`collect` operator to aggregate the posting location information from the multiple buckets/indices into a single dictionary\n",
    "posting_locs_list_anchor = partition_postings_and_write(postings_anchor,dir_n_anchor).collect()\n",
    "# merge the posting locations into a single dict \n",
    "super_posting_locs_anchor = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=dir_n_anchor):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_anchor[k].extend(v)\n",
    "\n",
    "#`collect` operator to aggregate the posting location information from the multiple buckets/indices into a single dictionary\n",
    "posting_locs_list_title = partition_postings_and_write(postings_title,dir_n_title).collect()\n",
    "print(posting_locs_list_title)\n",
    "# merge the posting locations into a single dict \n",
    "super_posting_locs_title = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix=dir_n_title):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs_title[k].extend(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_text = InvertedIndex(dir_n = dir_n_text)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_text.posting_locs = super_posting_locs_text\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_text.df = w2df_text_dict\n",
    "##add title-id list \n",
    "inverted_text.titles = titles_list\n",
    "##add DL\n",
    "inverted_text.DL = DL\n",
    "# write the global statrite_indexs out\n",
    "inverted_text.norm_dict = norm_dict\n",
    "inverted_text.write_index('.', 'index_text')\n",
    "# upload to gs\n",
    "index_src = \"index_text.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_text}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "BR3GqCyW7SeA",
   "metadata": {
    "id": "BR3GqCyW7SeA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[270, 250, 355, 295, 255, 340, 320, 325, 260, 305, 360, 315, 300, 290, 370, 345, 285, 265, 275, 330, 310, 280, 365, 335, 350, 301, 281, 331, 276, 306, 341, 351, 271, 316, 356, 371, 321, 326, 361, 256, 296, 336, 261, 266, 286, 251, 311, 291, 346, 366, 297, 307, 277, 347, 342, 367, 292, 282, 322, 267, 257, 272, 352, 327, 287, 332, 337, 362, 357, 252, 317, 302, 312, 262, 318, 353, 263, 313, 283, 298, 308, 368, 268, 358, 348, 333, 258, 253, 328, 273, 288, 303, 363, 338, 323, 278, 293, 248, 343, 254, 344, 274, 299, 349, 249, 364, 259, 369, 324, 359, 269, 354, 329, 319, 264, 334, 279, 289, 339, 314, 294, 309, 304, 284]\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_anchor = InvertedIndex(dir_n = dir_n_anchor)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_anchor.posting_locs = super_posting_locs_anchor\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_anchor.df = w2df_anchor_dict\n",
    "##add title-id list \n",
    "inverted_anchor.titles = titles_list\n",
    "##add DL\n",
    "inverted_anchor.DL = DL\n",
    "# write the global stats out\n",
    "inverted_anchor.write_index('.', 'index_anchor')\n",
    "# upload to gs\n",
    "index_src = \"index_anchor.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_anchor}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bkLAg0su8TRl",
   "metadata": {
    "id": "bkLAg0su8TRl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://index_text.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  6.0 MiB/  6.0 MiB]                                                \n",
      "Operation completed over 1 objects/6.0 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted_text = InvertedIndex(dir_n = dir_n_text)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_text.posting_locs = super_posting_locs_text\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_text.df = w2df_text_dict\n",
    "##add title-id list \n",
    "inverted_text.titles = titles_list\n",
    "##add DL\n",
    "inverted_text.DL = DL\n",
    "# write the global statrite_indexs out\n",
    "inverted_text.norm_dict = norm_dict\n",
    "inverted_text.write_index('.', 'index_text')\n",
    "# upload to gs\n",
    "index_src = \"index_text.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_text}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q3y7grjj8-Yj",
   "metadata": {
    "id": "q3y7grjj8-Yj"
   },
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_anchor = InvertedIndex(dir_n = dir_n_anchor)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_anchor.posting_locs = super_posting_locs_anchor\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_anchor.df = w2df_anchor_dict\n",
    "##add title-id list \n",
    "inverted_anchor.titles = titles_list\n",
    "##add DL\n",
    "inverted_anchor.DL = DL\n",
    "# write the global stats out\n",
    "inverted_anchor.write_index('.', 'index_anchor')\n",
    "# upload to gs\n",
    "index_src = \"index_anchor.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_anchor}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Y4oBXkL8xhA",
   "metadata": {
    "id": "-Y4oBXkL8xhA"
   },
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_title = InvertedIndex(dir_n = dir_n_title)\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_title.posting_locs = super_posting_locs_title\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_title.df = w2df_title_dict\n",
    "##add title-id list \n",
    "inverted_title.titles = titles_list\n",
    "##add DL\n",
    "inverted_title.DL = DL \n",
    "# write the global stats out\n",
    "inverted_title.write_index('.', 'index_title')\n",
    "# upload to gs\n",
    "index_src = \"index_title.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_title}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "    edges = pages.flatMapValues(lambda x :x).map(lambda x : (x[0],x[1].id)) .distinct()\n",
    "    vertices = edges .flatMap(lambda x: x).distinct().map(lambda x : Row(id=x))\n",
    "    return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(pages_links)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "pr = pr.collect()\n",
    "PR=defaultdict(float)\n",
    "for row in pr:\n",
    "    PR[row[0]]=row[1]\n",
    "# pr.repartition(1).write.csv(f'gs://{bucket_name}/{dir_n_pr}/pr', compression=\"gzip\")\n",
    "# upload to gs\n",
    "with open('PageRank.pkl', 'wb') as f:\n",
    "    pickle.dump(PR, f)\n",
    "index_src = \"PageRank.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_pr}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7001587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "# Using user page views (as opposed to spiders and automated traffic) for the \n",
    "# month of August 2021\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "p = Path(pv_path) \n",
    "pv_name = p.name\n",
    "pv_temp = f'{p.stem}-4dedup.txt'\n",
    "pv_clean = f'{p.stem}.pkl'\n",
    "# Download the file (2.3GB) \n",
    "!wget -N $pv_path\n",
    "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
    "# total number of page views (5). Then, remove lines with article id or page \n",
    "# view values that are not a sequence of digits.\n",
    "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Counter (dictionary) that sums up the pages views for the same \n",
    "# article, resulting in a mapping from article id to total page views.\n",
    "wid2pv = Counter()\n",
    "with open(pv_temp, 'rt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split(' ')\n",
    "        wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "# write out the counter as binary file (pickle it)\n",
    "with open(\"PageView.pkl\", 'wb') as f:\n",
    "    pickle.dump(wid2pv, f)\n",
    "index_src = \"PageView.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_pv}/{index_src}'\n",
    "!gsutil cp $index_src $index_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PageView.pkl\", 'wb') as f:\n",
    "    pickle.dump(wid2pv, f)\n",
    "index_src = \"PageView.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_pv}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=common_texts, window=5, min_count=1, workers=4)\n",
    "\n",
    "with open('Word2Vec.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "index_src = \"Word2Vec.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{dir_n_w2v}/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432e294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}