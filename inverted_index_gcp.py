# -*- coding: utf-8 -*-
"""new_inverted_index_gcp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FEiT5_WlUgStxmSDjLap_R3VLVCd7f3m

## Index implementation
"""

import sys
from collections import Counter, OrderedDict, defaultdict
import itertools
from itertools import islice, count, groupby
import os
import re
import numpy as np
from operator import itemgetter
from pathlib import Path
import pickle
from contextlib import closing
from collections import defaultdict
from contextlib import closing
from google.cloud import storage
bucket_name = "207437625"
BLOCK_SIZE = 1999998
# import gcsfs
"""inverted index title"""

class MultiFileWriter:
    """ Sequential binary writer to multiple files of up to BLOCK_SIZE each. """
    def __init__(self, base_dir, name, bucket_name,dir_n):
        self._base_dir = Path(base_dir)
        self._name = name
        self._file_gen = (open(self._base_dir/ f'{name}_{i:03}.bin', 'wb') 
                          for i in itertools.count())
        self._f = next(self._file_gen)
        # Connecting to google storage bucket. 
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket_name)
    
    def write(self, b,dir_n):
        locs = []
        while len(b) > 0:
            pos = self._f.tell()
            remaining = BLOCK_SIZE - pos
        # if the current file is full, close and open a new one.
            if remaining == 0:  
                self._f.close()
                self.upload_to_gcp(dir_n=dir_n)                
                self._f = next(self._file_gen)
                pos, remaining = 0, BLOCK_SIZE
            self._f.write(b[:remaining])
            locs.append((self._f.name, pos))
            b = b[remaining:]
        return locs

    def upload_to_gcp(self,dir_n):
        '''
            The function saves the posting files into the right bucket in google storage.
        '''
        file_name = self._f.name
        blob = self.bucket.blob(f"{dir_n}/{file_name}")
        blob.upload_from_filename(file_name)


    def close(self):
        self._f.close()


def download_bin_from_buck(f_name,dir_n):
    bucket = storage.Client().get_bucket('207437625')
    blob = bucket.get_blob(f"{dir_n}/{f_name}")
    blob.download_to_filename(f_name)
    return open(f_name, 'rb')
                          
class MultiFileReader:
    def __init__(self):
        self._open_files = {}

    def read(self, locs, n_bytes,n_dir):
        b = []
        for f_name, offset in locs:
            if f_name not in self._open_files:
                if os.path.exists(f_name):
                    self._open_files[f_name] = open(f"{f_name}", 'rb')
                else:
                    self._open_files[f_name] = download_bin_from_buck(f_name,n_dir)##delete in gcp
            f = self._open_files[f_name]
            f.seek(offset)
            n_read = np.min([n_bytes, BLOCK_SIZE - offset])
            b.append(f.read(n_read))
            n_bytes -= n_read
        return b''.join(b)

    # def read(self, locs, n_bytes,n_dir):
    #     b = []
    #     for loc in locs:
    #         f_name = loc[0]
    #         offset = loc[1]
    #         fs = gcsfs.GCSFileSystem(project='shirs-project-370310')
    #         with fs.open(f'{bucket_name}/{n_dir}/{f_name}') as f:
    #             f.seek(offset)
    #             n_read = min(n_bytes, BLOCK_SIZE - offset)
    #             b.append(f.read(n_read))
    #             n_bytes -= n_read
    #     return b''.join(b)
  
    def close(self):
        for f in self._open_files.values():
            f.close()

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()
        return False 
    
TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this 
                     # many bytes.
TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer


class InvertedIndex:  
    def __init__(self,dir_n, docs={}):
        self.DL = {}
        # stores document frequency per term
        self.df = Counter()
        # stores total frequency per term
        self.term_total = Counter()
        # stores posting list per term while building the index (internally), 
        # otherwise too big to store in memory.
        self._posting_list = defaultdict(list)
        # mapping a term to posting file locations, which is a list of 
        # (file_name, offset) pairs. Since posting lists are big we are going to
        # write them to disk and just save their location in this list. We are 
        # using the MultiFileWriter helper class to write fixed-size files and store
        # for each term/posting list its list of locations. The offset represents 
        # the number of bytes from the beginning of the file where the posting list
        # starts. 
        self.posting_locs = defaultdict(list)
        self.titles = None
        self.dir_n = dir_n
        self.norm_dict = None
    
        for doc_id, tokens in docs.items():
            self.add_doc(doc_id, tokens)

    def add_doc(self, doc_id, tokens):
        w2cnt = Counter(tokens)
        self.term_total.update(w2cnt)
        max_value = max(w2cnt.items(), key=operator.itemgetter(1))[1]    
        # frequencies = {key: value/max_value for key, value in frequencies.items()}
        for w, cnt in w2cnt.items():        
            self.df[w] = self.df.get(w, 0) + 1                
            self._posting_list[w].append((doc_id, cnt))


    def write(self, base_dir, name):
        #### POSTINGS ####
        self.posting_locs = defaultdict(list)
        with closing(MultiFileWriter(base_dir, name, self.dir_n)) as writer:###check if needs to be self
          # iterate over posting lists in lexicographic order
          for w in sorted(self._posting_list.keys()):
            self.write_a_posting_list(w, writer, sort=True)
        #### GLOBAL DICTIONARIES ####
        self._write_globals(base_dir, name)

    def _write_globals(self, base_dir, name):
        with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:
            pickle.dump(self, f) 

    def write_index(self, base_dir, name):
        self._write_globals(base_dir, name)

    @staticmethod
    def read_index(base_dir, name):
        with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:
            return pickle.load(f)

    @staticmethod
    def delete_index(base_dir, name):
        path_globals = Path(base_dir) / f'{name}.pkl'
        path_globals.unlink()
        for p in Path(base_dir).rglob(f'{name}_*.bin'):
            p.unlink()

    def posting_lists_iter(self):
        with closing(MultiFileReader()) as reader:
            for w, locs in self.posting_locs.items():
            # read a certain number of bytes into variable b
                b = reader.read(locs, self.df[w] * TUPLE_SIZE)
                posting_list = []
            # convert the bytes read into `b` to a proper posting list.

            for i in range(self.df[w]):
                doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')
                tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')
                posting_list.append((doc_id, tf))
        
        yield w, posting_list
    @staticmethod
    def write_a_posting_list(b_w_pl, bucket_name,dir_n):
        posting_locs = defaultdict(list)
        bucket_id, list_w_pl = b_w_pl
        with closing(MultiFileWriter(".", bucket_id, bucket_name,dir_n)) as writer:
            for w, pl in list_w_pl:
                # convert to bytes
                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')
                              for doc_id, tf in pl])
                # write to file(s)
                locs = writer.write(b,dir_n)
                # save file locations to index
                posting_locs[w].extend(locs)
            writer.upload_to_gcp(dir_n)
            InvertedIndex._upload_posting_locs(bucket_id, posting_locs, bucket_name,dir_n)
        return bucket_id

    @staticmethod
    def _upload_posting_locs(bucket_id, posting_locs, bucket_name,dir_n):
        with open(f"{bucket_id}_posting_locs.pickle", "wb") as f:
            pickle.dump(posting_locs, f)
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob_posting_locs = bucket.blob(f"{dir_n}/{bucket_id}_posting_locs.pickle")
        blob_posting_locs.upload_from_filename(f"{bucket_id}_posting_locs.pickle")

